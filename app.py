"""
NPathQ: A Neuropathology Report QA Agent
====================================================

This **fully offline** Gradio application lets you chat with the contents of a
PDF using any locally available causalâ€‘LM (default: *Metaâ€‘Llamaâ€‘3.1â€‘8Bâ€‘Instruct*).
It is aimed at GPU workstations/clusters where privacy or airâ€‘gap constraints
rule out cloud APIs.

Workflow
--------
1. **Upload a PDF**.
2. The file is parsed to plain text by `docling`, a unified document understanding
   framework from the Linux Foundation AI & Data ecosystem. It supports page layout,
   OCR, and multiple export formats.
3. Text is split into *overlapping* chunks (â‰ˆ1k tokens) â€“ the overlap prevents
   fuzzy cutâ€‘offs from hiding facts that straddle chunk boundaries â€“ and each
   chunk is embedded with *allâ€‘MiniLMâ€‘L6â€‘v2*.
4. Chunks are stored in a local FAISS index.
5. A `ConversationalRetrievalChain` feeds the most relevant chunks plus the
   **system prompt** (loaded from *system_prompt.md*) into your chosen LLM.
   Earlier turns are appended for onâ€‘session memory until the PDF changes.
6. Answers stream to the chat UI, each suffixed with a laboratory watermark.

CLI Usage
---------
```bash
# default llama 3.1 8B
python pdf_qa_app.py

# pick another model & prompt
python pdf_qa_app.py --model mistralai/Mistral-7B-Instruct-v0.3 \
                    --prompt custom_prompt.md

# override port or expose in docker-compose
python pdf_qa_app.py --port 8501
```
"""

import argparse, os
from pathlib import Path
from typing import Generator, List, Tuple

import gradio as gr
import torch
from docling.document_converter import DocumentConverter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import VLLM
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain

AGENT_NAME = "NPathQ"
WATERMARK = (
    "\n\nâ€” This answer is generated by NPathQ, an agent from Haining Wang@Su's "
    "Lab, IU School of Medicine. Contact hw56@iu.edu for assistance."
)

# ------------------------------------------------------------------ helpers
def _device() -> str:
    return "cuda:1" if torch.cuda.device_count() > 1 else "cuda:0" if torch.cuda.is_available() else "cpu"

def load_llm(model_id: str, max_new_tokens: int = 1024):
    return VLLM(
        model=model_id,
        max_new_tokens=max_new_tokens,
        trust_remote_code=True,
        tensor_parallel_size=1,
        dtype="bfloat16",
        streaming=True,
    )

def load_system_prompt(path: Path) -> str:
    return path.read_text().strip()

def pdf_to_text(pdf: Path) -> str:
    txt = DocumentConverter().convert(str(pdf)).document.export_to_text()
    return txt

def make_vector_store(text: str):
    chunks = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100).create_documents([text])
    embedder = HuggingFaceEmbeddings("sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": _device()})
    return FAISS.from_documents(chunks, embedder)

def make_chain(vstore: FAISS, system_prompt: str):
    retriever = vstore.as_retriever(search_kwargs={"k": 4})
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=f"{system_prompt}\n\n{{context}}\n\nUser question: {{question}}\nAnswer:",
    )
    return ConversationalRetrievalChain.from_llm(
        llm=LLM,  # global singleton
        retriever=retriever,
        return_source_documents=False,
        chain_type_kwargs={"prompt": prompt},
    )

# ------------------------------------------------------------------ gradio callbacks
def upload_pdf(pdf_file: gr.File, state: dict):
    if pdf_file is None:
        raise gr.Error("please upload a pdf first.")
    text = pdf_to_text(Path(pdf_file.name))
    state["chain"] = make_chain(make_vector_store(text), SYSTEM_PROMPT)
    state["history"] = []
    return [["system", "PDF parsed. Ask away!"]], state

def answer_question(message: str, state: dict) -> Generator:
    if "chain" not in state:
        raise gr.Error("upload a pdf first.")
    history: List[Tuple[str, str]] = state.get("history", [])
    chain = state["chain"]

    # LangChain streaming generator
    for chunk in chain.stream({"question": message, "chat_history": history}):
        partial = chunk["answer"]
        yield [[q, a] for q, a in history] + [[message, partial + "â–Œ"]], state

    full_answer = chunk["answer"] + WATERMARK
    history.append((message, full_answer))
    state["history"] = history
    yield [[q, a] for q, a in history], state

# ------------------------------------------------------------------ UI
def build_ui(port: int):
    with gr.Blocks(css="body {font-family: 'Inter', sans-serif;}") as demo:
        gr.Markdown(f"# ðŸ§  {AGENT_NAME}: Neuro-Pathology Q-A")
        # state stores only lightweight python objects
        state = gr.State({"history": []})

        with gr.Row():
            pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"], interactive=True)
            reset_btn = gr.Button("Reset session")

        chat = gr.Chatbot(height=480)
        inp = gr.Textbox(lines=1, placeholder="Type your questionâ€¦")

        pdf_input.change(upload_pdf, [pdf_input, state], [chat, state])
        reset_btn.click(lambda: ([], {"history": []}), None, [chat, state])
        inp.submit(answer_question, [inp, state], [chat, state], stream=True)

        gr.Markdown(f"<small>running on **{_device()}** | model: **{MODEL_ID}**</small>")
    demo.launch(server_name="0.0.0.0", server_port=port, share=False)

# ------------------------------------------------------------------ main
if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--model", default="meta-llama/Meta-Llama-3.1-8B-Instruct")
    p.add_argument("--prompt", default="system_prompt.md")
    p.add_argument("--port", type=int, default=7860)
    args = p.parse_args()

    MODEL_ID = args.model
    SYSTEM_PROMPT = load_system_prompt(Path(args.prompt))
    LLM = load_llm(MODEL_ID)  # singleton, kept outside gr.State

    build_ui(args.port)
