"""
NPathQ: A Neuropathology Report QA Agent
====================================================

This **fully offline** Gradio application lets you chat with the contents of a
PDF using any locally available causal‑LM (default: *Meta‑Llama‑3.1‑8B‑Instruct*).
It is aimed at GPU workstations/clusters where privacy or air‑gap constraints
rule out cloud APIs.

Workflow
--------
1. **Upload a PDF**.
2. The file is parsed to plain text by `docling`, a unified document understanding
   framework from the Linux Foundation AI & Data ecosystem. It supports page layout,
   OCR, and multiple export formats.
3. Text is split into *overlapping* chunks (≈1k tokens) – the overlap prevents
   fuzzy cut‑offs from hiding facts that straddle chunk boundaries – and each
   chunk is embedded with *all‑MiniLM‑L6‑v2*.
4. Chunks are stored in a local FAISS index.
5. A `ConversationalRetrievalChain` feeds the most relevant chunks plus the
   **system prompt** (loaded from *system_prompt.md*) into your chosen LLM.
   Earlier turns are appended for on‑session memory until the PDF changes.
6. Answers stream to the chat UI, each suffixed with a laboratory watermark.

CLI Usage
---------
```bash
# default llama 3.1 8B
python pdf_qa_app.py

# pick another model & prompt
python pdf_qa_app.py --model mistralai/Mistral-7B-Instruct-v0.3 \
                    --prompt custom_prompt.md

# override port or expose in docker-compose
python pdf_qa_app.py --port 8501
```
"""

import argparse
from pathlib import Path
from typing import List, Tuple

import gradio as gr
import torch
from docling.document_converter import DocumentConverter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import VLLM
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain

AGENT_NAME = "NPathQ"
WATERMARK = (
    "\n\n— This answer is generated by NPathQ, an agent from Haining Wang@Su's "
    "Lab, IU School of Medicine. Contact hw56@iu.edu for assistance."
)

# ------------------------------ helpers -----------------------------------
def _device() -> str:
    return "cuda:1" if torch.cuda.device_count() > 1 else "cuda:0" if torch.cuda.is_available() else "cpu"

def load_llm(model_id: str, max_new: int = 1024):
    return VLLM(
        model=model_id,
        max_new_tokens=max_new,
        trust_remote_code=True,
        tensor_parallel_size=1,
        dtype="bfloat16",
        streaming=False,          # ← streaming off for stability
    )

def pdf_to_text(pdf: Path) -> str:
    return DocumentConverter().convert(str(pdf)).document.export_to_text()

def vector_store(text: str):
    chunks = RecursiveCharacterTextSplitter(chunk_size=1024,
                                            chunk_overlap=100).create_documents([text])
    embed = HuggingFaceEmbeddings("sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": _device()})
    return FAISS.from_documents(chunks, embed)

def qa_chain(vstore, system_prompt: str):
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=f"{system_prompt}\n\n{{context}}\n\nUser question: {{question}}\nAnswer:",
    )
    return ConversationalRetrievalChain.from_llm(
        llm=LLM,
        retriever=vstore.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=False,
        chain_type_kwargs={"prompt": prompt},
    )

# ------------------------------ gradio callbacks --------------------------
def upload_pdf(pdf: gr.File, state: dict):
    if pdf is None:
        raise gr.Error("Please upload a PDF first.")
    txt = pdf_to_text(Path(str(pdf)))
    state["chain"] = qa_chain(vector_store(txt), SYSTEM_PROMPT)
    state["history"] = []
    return [{"role": "system", "content": "PDF parsed. Ask away!"}], state

def answer(msg: str, state: dict):
    if "chain" not in state:
        raise gr.Error("Upload a PDF first.")
    hist: List[Tuple[str, str]] = state.get("history", [])
    chain = state["chain"]
    resp = chain({"question": msg, "chat_history": hist})["answer"] + WATERMARK
    hist.append((msg, resp))
    state["history"] = hist
    messages = [{"role": "user", "content": q} if i % 2 == 0 else {"role": "assistant", "content": a}
                for i, (q, a) in enumerate(sum(([m] for m in hist), []))]
    return messages, state

# ------------------------------ UI ----------------------------------------
def build_ui(port: int):
    with gr.Blocks(css="body {font-family: 'Inter', sans-serif;}") as demo:
        gr.Markdown(f"# 🧠 {AGENT_NAME}: Neuro-Pathology Q-A")
        state = gr.State({})

        with gr.Row():
            pdf_file = gr.File(label="Upload PDF", file_types=[".pdf"])
            reset_btn = gr.Button("Reset session")

        chat = gr.Chatbot(height=480, type="messages")
        box  = gr.Textbox(lines=1, placeholder="Type your question…")

        pdf_file.change(upload_pdf, [pdf_file, state], [chat, state])
        reset_btn.click(lambda: ([], {}), None, [chat, state])
        box.submit(answer, [box, state], [chat, state])

        gr.Markdown(f"<small>device **{_device()}** | model **{MODEL_ID}**</small>")

    demo.launch(server_name="0.0.0.0", server_port=port, share=False)

# ------------------------------ main --------------------------------------
if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--model", default="meta-llama/Meta-Llama-3.1-8B-Instruct")
    p.add_argument("--prompt", default="system_prompt.md")
    p.add_argument("--port", type=int, default=7860)
    args = p.parse_args()

    MODEL_ID = args.model
    SYSTEM_PROMPT = Path(args.prompt).read_text().strip()
    LLM = load_llm(MODEL_ID)

    build_ui(args.port)
