"""
NPathQ: A Neuropathology Report QA Agent
====================================================

This **fully offline** Gradio application lets you chat with the contents of a
PDF using any locally available causalâ€‘LM (default: *Metaâ€‘Llamaâ€‘3.1â€‘8Bâ€‘Instruct*).
It is aimed at GPU workstations/clusters where privacy or airâ€‘gap constraints
rule out cloud APIs.

Workflow
--------
1. **Upload a PDF**.
2. The file is parsed to plain text by `docling`, a unified document understanding
   framework from the Linux Foundation AI & Data ecosystem. It supports page layout,
   OCR, and multiple export formats.
3. Text is split into *overlapping* chunks (â‰ˆ1k tokens) â€“ the overlap prevents
   fuzzy cutâ€‘offs from hiding facts that straddle chunk boundaries â€“ and each
   chunk is embedded with *allâ€‘MiniLMâ€‘L6â€‘v2*.
4. Chunks are stored in a local FAISS index.
5. A `ConversationalRetrievalChain` feeds the most relevant chunks plus the
   **system prompt** (loaded from *system_prompt.md*) into your chosen LLM.
   Earlier turns are appended for onâ€‘session memory until the PDF changes.
6. Answers stream to the chat UI, each suffixed with a laboratory watermark.

CLI Usage
---------
```bash
# default llama 3.1 8B
python pdf_qa_app.py

# pick another model & prompt
python pdf_qa_app.py --model mistralai/Mistral-7B-Instruct-v0.3 \
                    --prompt custom_prompt.md

# override port or expose in docker-compose
python pdf_qa_app.py --port 8501
```

Dependencies
------------
* gradio >=â€¯4.28 Â· langchain >=â€¯0.2 Â· transformers >=â€¯4.41 Â· accelerate >=â€¯0.30
* sentenceâ€‘transformers Â· faissâ€‘cpu (or faissâ€‘gpu) Â· doclingâ€‘project Â· vllm
"""

import argparse
import os
from pathlib import Path
from typing import List, Tuple

import gradio as gr
import torch
from langchain.docstore.document import Document
from docling.document_converter import DocumentConverter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import VLLM
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain

AGENT_NAME = "NPathQ"
WATERMARK = (
    "\n\nâ€” This answer is generated by NPathQ, an agent from Haining Wang@Su's "
    "Lab, IU School of Medicine. Contact hw56@iu.edu for assistance."
)

# ---------------------------------------------------------------------------
# device & model helpers
# ---------------------------------------------------------------------------

def _select_device() -> str:
    if torch.cuda.is_available():
        return "cuda:1" if torch.cuda.device_count() > 1 else "cuda:0"
    return "cpu"


def load_llm(model_name: str, max_new_tokens: int = 10240):
    return VLLM(
        model=model_name,
        max_new_tokens=max_new_tokens,
        trust_remote_code=True,
        tensor_parallel_size=1,
        dtype="bfloat16",
        streaming=True
    )

# ---------------------------------------------------------------------------
# system prompt handling
# ---------------------------------------------------------------------------

def load_system_prompt(path: Path) -> str:
    if not path.exists():
        raise FileNotFoundError(f"system prompt file not found: {path}")
    return path.read_text(encoding="utf-8").strip()

# ---------------------------------------------------------------------------
# document handling
# ---------------------------------------------------------------------------

def parse_pdf_to_text(pdf_path: Path) -> str:
    converter = DocumentConverter()
    result = converter.convert(str(pdf_path))
    return result.document.export_to_text()


def build_vector_store(text: str):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
    docs = splitter.create_documents([text])
    embedder = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": _select_device()},
    )
    return FAISS.from_documents(docs, embedder)


def build_qa_chain(vstore: FAISS, llm, system_prompt: str):
    retriever = vstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=f"{system_prompt}\n\n{{context}}\n\nUser question: {{question}}\nAnswer:",
    )
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=False,
        chain_type_kwargs={"prompt": prompt},
    )

# ---------------------------------------------------------------------------
# gradio callbacks
# ---------------------------------------------------------------------------

def upload_pdf(pdf_file: gr.File, state: dict):
    if pdf_file is None:
        raise gr.Error("please upload a pdf first.")

    text = parse_pdf_to_text(Path(pdf_file.name))
    vstore = build_vector_store(text)
    state["chain"] = build_qa_chain(vstore, state["llm"], state["system_prompt"])
    state["history"] = []

    return (
        [["system", "PDF parsed. Ask away!"]],
        state,
    )


def answer_question(message: str, state: dict):
    if "chain" not in state:
        raise gr.Error("upload a pdf first.")

    chat_history: List[Tuple[str, str]] = state.get("history", [])
    chain = state["chain"]
    result = chain({"question": message, "chat_history": chat_history})
    answer = result["answer"] + WATERMARK
    chat_history.append((message, answer))
    state["history"] = chat_history
    return [[q, a] for q, a in chat_history], state

# ---------------------------------------------------------------------------
# ui construction
# ---------------------------------------------------------------------------

def build_ui(model_name: str, prompt_path: Path):
    system_prompt = load_system_prompt(prompt_path)
    with gr.Blocks(css="body {font-family: 'Inter', sans-serif;}") as demo:
        gr.Markdown(f"# ðŸ§  {AGENT_NAME}: Neuro-Pathology Q-A")
        state = gr.State({
            "llm": load_llm(model_name),
            "system_prompt": system_prompt,
        })

        with gr.Row():
            pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"], interactive=True)
            clear_btn = gr.Button("Reset")

        chatbot = gr.Chatbot(elem_id="chatbot", height=480)
        text_input = gr.Textbox(placeholder="Type your questionâ€¦", lines=1)

        pdf_input.change(upload_pdf, inputs=[pdf_input, state], outputs=[chatbot, state])
        clear_btn.click(lambda s: ([], {k: s[k] for k in ("llm", "system_prompt")}),
                       inputs=state, outputs=[chatbot, state])
        text_input.submit(answer_question,
                          inputs=[text_input, state],
                          outputs=[chatbot, state],
                          stream=True)

        gr.Markdown(
            f"<small>running on **{_select_device()}** | model: **{model_name}** | "
            f"prompt: **{prompt_path.name}**</small>"
        )
    return demo

# ---------------------------------------------------------------------------
# cli entrypoint
# ---------------------------------------------------------------------------

def parse_args():
    parser = argparse.ArgumentParser(description="run the pdfâ€‘qa gradio app locally")
    parser.add_argument("--model", default=os.getenv("MODEL_NAME", "meta-llama/Meta-Llama-3.1-8B-Instruct"),
                        help="hf model id or local path of the causalâ€‘lm")
    parser.add_argument("--prompt", default=os.getenv("PROMPT_PATH", "system_prompt.md"),
                        help="markdown file containing the system prompt")
    parser.add_argument("--port", type=int, default=7860, help="gradio server port")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    ui = build_ui(args.model, Path(args.prompt))
    ui.launch(server_name="0.0.0.0", server_port=args.port, share=False)
